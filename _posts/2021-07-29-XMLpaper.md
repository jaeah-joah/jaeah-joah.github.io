---
title: "[논문 리뷰] XML-Cross-lingual Language Model Pretrainig"
date: 2021-07-29
last_modified_at: 2021-08-01

categories: 
  - Paper Review
tags:
  - [자연어처리, NLP, 논문리뷰, XML, Multilingual, Cross-lingual]

toc: true
toc_sticky: true

---

# Introduction

- 본 논문이 발표되었을 당시, general-purpose sentence representations에 대한 관심은 증가했지만, 해당 분야의 연구는 기본적으로 단일 언어로 이루어졌으며 주로 영어 벤치마크에 집중됨
- 따라서 최근에 여러 언어로 된 cross-lingual sentence representations를 학습하고 평가하는 방식은 영어 중심적인 편향을 해소하고자 함
- 이처럼 영어에 집중된 연구를 여러 언어로 확장하고, cross-lingual pretraining의 효과를 입증하려고 함

## Cross-lingual transfer learning

- 영어 중심적인 모델에서 다양한 언어로 확장시키는 방법은 cross-lingual transfer learning이라고 하는데, 이는 풍부한 양의 source language를 학습시켜 새로운 언어에 태스크를 적용할 수 있는 방법임
- 본 논문에서는 보편적인 cross-lingual encoders를 구축함으로써 어떤 문장이든 공유 임베딩 스페이스에 인코딩할 수 있다고 제안

## XLM(Cross-lingual Language Model)

1. 단일 언어를 사용한 새로운 unsupervised learning 제시
2. 병렬 데이터를 이용한 cross-lingual language model을 적용한 supervised learning 제시 

# Methodology

## Causal Language Modeling(CLM)

- 한국어로 인과관계 언어모델
- 이전에 등장한 단어들이 다음에 등장하는 단어에 영향을 주는 구조

    $$P (wt|w1, . . . , wt−1, θ)$$

    - 트랜스포머
    - RNN based model, LSTM
- 이 기법은 cross-lingual setting에 확장적이지는 않음

## Masked Language Modeling(MLM)

![스크린샷_2021-07-28_오전_1 15 29](https://user-images.githubusercontent.com/76445666/127746818-26aad188-06bc-4b17-8612-911653d1a981.png)

- BERT와 거의 유사
- 각 문장에 있는 토큰중 15%만 [MASK]ing 됨
- BERT와 딱 하나 차이점은 input 문장이 연속적인 stream text가 들어간다는 것
- BERT에서 segment embeddings → XLM에서 language embeddings

CLM, MLM 모두

- unsupervised leanring
- monolingual data

## Translation Language Modeling(TLM)

![스크린샷_2021-07-28_오전_1 43 14](https://user-images.githubusercontent.com/76445666/127746849-c20d969e-0381-465e-b593-addd1e5a4258.png)

- parallel data를 사용하는 supervised learning
- 번역을 하기 위해 두가지 언어로 이루어진 병렬데이터셋을 사용
- MLM의 확장
- 소스 문장과 타겟 문장에서 모두 무작위로 마스킹을 함
- 영어 문장에서 마스킹된 단어를 예측하기 위해 모델은 주변 영어 단어 또는 프랑스어 번역을 볼 수 있다. 모델로 하여금 영어와 프랑스어 표현을 정렬하도록 하면서
- 특히, 영어가 마스킹된 영어 단어를 추론하기에 충분하지 않은 경우, 모델은 프랑스어 context를 활용할 수 있음
- 정렬을 용이하게 하기 위해 타겟 문장의 위치도 재설정할 수 있다.? 타겟 문장이 왼쪽에 들어가도 오른쪽에 들어가도 된다는 것 같음

# Experiments

## Cross-lingual classification

![스크린샷_2021-07-28_오전_2 34 22](https://user-images.githubusercontent.com/76445666/127746894-8c0250db-c426-4b07-a3d5-5e1ebfe957e2.png)

Cross-lingual natural language inference (XNLI) dataset

- Cross-lingual natural language inference (XNLI) dataset 을 사용
- MultiNLI라는 15개국 언어로 확장시킨 것이 XNLI (inference labelling task)
- 전제와 가설이 entailment(a가 참일경우 b도 참), neutral(a로부터 b추론 불가), contradiction(a참일때 b거짓)한지 분류

![스크린샷_2021-07-28_오전_2 36 49](https://user-images.githubusercontent.com/76445666/127746995-cfbca845-d23f-402b-90e9-bf3f424c614d.png)

- Pre-training 이후에 Transformer 뒷단에 linear classifer 하나를 두고 MultiNLI 학습 셋으로 fine-tuning
- 그 후에 15개 언어에 대해 XNLI를 테스트

### Translate-Train
1. 영어 NLI 데이터를 각국 언어로 번역
2. 각국 언어로 번역된 데이터로 fine-tuning
3. 모델에 각국 언어로 된 XNLI test 데이터로 평가

### Translate-Test
1. 영어 NLI train 데이터로 fine-tuning
2. 각국 언어로 된 XMLI test 데이터를 영어로 번역
3. 모델에 영어로 번역된 XNI test 데이터로 평가

### Cross-lingual Classification
- 영어 NLI 데이터로 학습을 하고, 각국언어인 MultiNLI로 평가를 한 결과
- MLM만 사용한 XLM은 제로샷 cross lingual classification
- MLM+TLM이 'translate-train'했을때의 성능과 거의 유사

## Unsupervised Machine Translation

![Untitled](https://user-images.githubusercontent.com/76445666/127747054-6e446810-59ae-4083-9d35-948265d9f179.png)

- 이전 논문에서 계속하고 있던 연구라고 함
- Unsupervised neural machine translation(UNMT)에서 초기 cross-lingual word embedding의 품질이 매우 중요함.
- 초기 word-embedding을 룩업 테이블을 사용하는 대신 cross-lingual model로 전체 인코더와 디코더를 pre-training함
- WMT 14 En-Fr, WMT 16 En-Dr, WMT 16 En - Ro에 대해 테스트
- 인코더와 디코더를 프리트레인하는데, MLM-MLM의 성능이 제일 좋음

## Supervised Machine Translation

![스크린샷_2021-07-28_오후_3 20 24](https://user-images.githubusercontent.com/76445666/127747134-2877ce32-3212-4841-9bf5-b7613681b8da.png)

- 각각 로마니아어에서 영어로 단일방향, 양방향, 양방향 학습에 backtranslation기법을 적용한것임
- CLM을 적용한 것 보다 MLM을 적용한 것이 성능이 좋았음

## Low-resource language modeling

![스크린샷_2021-07-28_오후_3 20 24](https://user-images.githubusercontent.com/76445666/127747145-352c59ec-4981-43ff-a573-e5829eba99b5.png)

- 자원이 적은 언어의 경우, 유사하지만 자원이 많은 언어의 데이터를 활용할 수 있음
- Wikipedia에 100k 문장 정도의 네팔어가 있고 힌디어는 약 6배 정도 됨

    두 개의 언어는 약 80% 정도의 BPE vocab을 공유(100k subword units)

- 네팔어, 네팔어+영어, 네팔어+힌디어, 네팔어+영어+힌디어로 각각 파인튜닝하여, 네팔어의 perplexity를 계산
- perplexity가 낮은 것이 좋은 모델

## Unsupervised cross-lingual word embeddings

![스크린샷_2021-07-28_오후_4 38 42](https://user-images.githubusercontent.com/76445666/127747168-0dc50b4a-32e3-4934-9b1f-10eba7418338.png)

/*MUSE*/

![스크린샷_2021-07-28_오후_3 29 58](https://user-images.githubusercontent.com/76445666/127747177-5be6d826-ddad-4da6-be43-e490c23a76cd.png)

- MUSE:프리트레인하고 훈련된 임베딩 x에 선형변환 y에 근사하도록하면 두 단어의 유사도를 계산
- Concat:Source language + Target language concat 하여 Fastext돌린 것
- XLM:TLM으로 학습시킨 것
- SemEval'17 :cross lingual similarity task 점수계산

# Conclusion

1. cross-lingual language modeling을 사용하여 cross-lingual representations을 학습하기 위한 새로운 unsupervised method를 제시하고, 두 가지의 monolingual pretraining 목표를 조사함
2. parallel data를 사용할 수 있을 때, cross-lingual pretraining을 개선하는 새로운 supervised learning을 제시
3. XLM은 이전의 cross-lingual classification, unsupervised machine translation, supervised machine translation 의 성능을 뛰어넘었음
4. cross-lingual language models이 low-resource languages 복잡성에 상당한 개선을 제공함을 보여줌
  
## 개인적 견해

- pararell corpora로 이루어진 데이터라면 확실히 TLM기법이 효과적일 것 같음
- TLM이 이 논문의 핵심이라고 할 수 있는데, TLM으로 pre-training한 결과가 적어 아쉬움


<참고사이트>

[https://keep-steady.tistory.com/41](https://keep-steady.tistory.com/41)

[https://yhdosu.tistory.com/entry/Cross-lingual-Language-Model-Pre-training](https://yhdosu.tistory.com/entry/Cross-lingual-Language-Model-Pre-training)

[https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5](https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5)

[https://www.youtube.com/watch?v=aRwvwdfK0cA](https://www.youtube.com/watch?v=aRwvwdfK0cA)
